<!doctype html>
<!--[if lt IE 7]><html class="no-js lt-ie9 lt-ie8 lt-ie7" lang="en"> <![endif]-->
<!--[if (IE 7)&!(IEMobile)]><html class="no-js lt-ie9 lt-ie8" lang="en"><![endif]-->
<!--[if (IE 8)&!(IEMobile)]><html class="no-js lt-ie9" lang="en"><![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en"><!--<![endif]-->
<head>
{% include _head.html %}
<style>
  .bottom-three {
     margin-bottom: 1.5cm;
   }
  .bottom-small {
     margin-bottom: 1cm;
   }
  .publication_entry{
  background-color: #eeeeee;
  border: #dddddd 3px solid;
  padding:2px 6px 4px 6px;
  color: #555555;
  max-width:100%;
  max-height:100%;
}
.desc{
  float:right;
  width:67%;
}
.desc p{
  margin-top:0;
}
.clear{
  clear:both;
}
.title_font{
  color: #cc0000; font-family: 'Lato', sans-serif; font-size: 16px; font-weight: bold; line-height: 20px; margin-bottom: 30px; margin-right: 5px; margin-top: 5px; margin-left: 5px;
}
.body_font{
  color: gray; font-family: 'helvetica'; font-size: 12px; font-weight: bold; line-height: 17px;
  text-align: justify;
  text-justify: inter-word;
  margin-right: 5px; margin-top: -20px; margin-left: 5px; margin-bottom: 5px;
}
.authors_font{
  color: gray; font-family: 'helvetica'; font-size: 11px; font-weight: bold; line-height: 17px; margin-right: 11px; margin-left: 5px;}

  /*************************************
 The box that contain BibTeX code
 *************************************/
div.noshow { display: none; }
div.bibtex {
	margin-right: 0%;
	margin-top: 1.2em;
	margin-bottom: 1em;
	border: #dddddd 3px solid;
	padding: 0em 1em;
	background: #ffffff;}
div.bibtex pre { font-size: 75%; overflow: auto;  width: 100%; padding: 0em 0em;}
}
</style>
</head>
<script type="text/javascript">
    <!--
    // Toggle Display of BibTeX
    function toggleBibtex(articleid) {
        var bib = document.getElementById('bib_'+articleid);
        if (bib) {
            if(bib.className.indexOf('bibtex') != -1) {
                bib.className.indexOf('noshow') == -1?bib.className = 'bibtex noshow':bib.className = 'bibtex';
            }
        } else {
            return;
        }
    }
-->
    </script>
<body class="page">

{% include _browser-upgrade.html %}

{% include _navigation.html %}

{% if page.image.feature %}
  <div class="image-wrap">
  <img src=
    {% if page.image.feature contains 'http://' %}
      "{{ page.image.feature }}"
    {% elsif page.image.feature contains 'https://' %}
      "{{ page.image.feature }}"
    {% else %}
      "{{ site.url }}/images/{{ page.image.feature }}"
    {% endif %}
  alt="{{ page.title }} feature image">
  {% if page.image.credit %}
    <span class="image-credit">Photo Credit: <a href="{{ page.image.creditlink }}">{{ page.image.credit }}</a></span>
  {% endif %}
  </div><!-- /.image-wrap -->
{% endif %}
<div id="main" role="main">
  <div class="article-author-side">
    {% include _author-bio.html %}
  </div>
  <article>

		<div class="publication_entry">
	<img src="/images/publications/Multiview_CVPR2020.jpg" border= "5px solid" width="250px" height="150px" alt="Teaser" align="left" style="margin:5px 8px 8px 5px; border: #aaaaaa 3px solid">
	<p class="title_font"> Learning multiview 3D point cloud registration </p>
  <p class="authors_font"> Zan Gojcic,  <a href="https://gseg.igp.ethz.ch/people/scientific-assistance/caifa-zhou.html" target="_blank"> Caifa Zhou, </a> <a href="https://igp.ethz.ch/personen/person-detail.html?persid=186562" target="_blank"> Jan D. Wegner,</a>
    <a href="https://geometry.stanford.edu/member/guibas/" target="_blank">  Leonidas J. Guibas, </a> <a href="http://tbirdal.me/" target="_blank">  Tolga Birdal </a> </p>
	<p class="authors_font"> IEEE Computer Vision and Pattern Recognition (CVPR), 2020 </p>
	<p class="body_font"> We present a novel, end-to-end learnable, multiview 3Dpoint cloud registration algorithm. Registration of multiple scans typically follows a two-stage pipeline:  the initial pairwise alignment and the globally consistent refinement.The former is often ambiguous due to the low overlap of neighboring point clouds, symmetries and repetitive sceneparts. Therefore, the latter global refinement aims at establishing the cyclic consistency across multiple scans and helps in resolving the ambiguous cases. In this paper we propose, to the best of our knowledge, the first end-to-end algorithm for joint learning of both parts of this two-stage problem. Experimental evaluation on well accepted benchmark datasets shows that our approach outperforms the state-of-the-art by a significant margin, while being end-to-end trainable and computationally less costly. Moreover, we present a detailed analysis and an ablation study that validatethe novel components of our approach.</p>
	<p style="font-family: 'Lato'; sans-serif; font-size: 14px; font-weight: bold; line-height: 16px; margin-top: 10px; margin-left: 5px;; margin-bottom: 5px"> <a href="https://arxiv.org/abs/2001.05119"> <i class="fa fa-file-pdf-o" style="font-size:24px;color:red"></i> PDF &nbsp&nbsp </a> <a href="https://github.com/zgojcic/3D_multiview_reg"> <i class="fa fa-github" style="font-size:24px"></i> Code &nbsp&nbsp</a> 
    <a href="javascript:toggleBibtex('gojcic2020multiview')"> BibTeX </a> </p>
  </div>
		<div id="bib_gojcic2020multiview" class="bibtex noshow">
<pre>
@inproceedings{gojcic2020multiview,
  title={Learning multiview 3D point cloud registration},
  author={Gojcic, Zan and Zhou, Caifa and Wegner, Jan D and Guibas, Leonidas J and Birdal, Tolga},
  booktitle={IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2020}
}
</pre>
</div>
<br>
	<div class="publication_entry">
	<img src="/images/publications/3DSmoothNet.jpg" border= "5px solid" width="250px" height="100px" alt="Teaser" align="left" style="margin:5px 8px 8px 5px; border: #aaaaaa 3px solid">
	<p class="title_font"> The Perfect Match: 3D Point Cloud Matching with Smoothed Densities</p>
	<p class="authors_font"> Zan Gojcic,  <a href="https://gseg.igp.ethz.ch/people/scientific-assistance/caifa-zhou.html" target="_blank"> Caifa Zhou, </a> <a href="https://igp.ethz.ch/personen/person-detail.html?persid=186562" target="_blank">  Jan D. Wegner</a>,  <a href="https://gseg.igp.ethz.ch/people/group-head/prof-dr--andreas-wieser.html" target="_blank">  Andreas Wieser</a></p>
	<p class="authors_font"> IEEE Computer Vision and Pattern Recognition (CVPR), 2019 </p>
	<p class="body_font"> We propose 3DSmoothNet, a full workflow to match 3D point clouds with a siamese deep learning architecture and fully convolutional layers using a voxelized smoothed density value (SDV) representation. The latter is computed per interest point and aligned to the local reference frame (LRF) to achieve rotation invariance. Our compact, learned, rotation invariant 3D point cloud descriptor achieves 94.9% average recall on the 3DMatch benchmark data set, outperforming the state-of-the-art by more than 20 percent points with only 32 output dimensions. This very low output dimension allows for near realtime correspondence search with 0.1 ms per feature point on a standard PC. Our approach is sensor- and sceneagnostic because of SDV, LRF and learning highly descriptive features with fully convolutional layers. We show that 3DSmoothNet trained only on RGB-D indoor scenes of buildings achieves 79.0% average recall on laser scans of outdoor vegetation, more than double the performance of our closest, learning-based competitors.</p>
	<p style="font-family: 'Lato'; sans-serif; font-size: 14px; font-weight: bold; line-height: 16px; margin-top: 10px; margin-left: 5px;; margin-bottom: 5px"> <a href="https://arxiv.org/pdf/1811.06879.pdf"> <i class="fa fa-file-pdf-o" style="font-size:24px;color:red"></i> PDF &nbsp&nbsp </a> <a href="https://github.com/zgojcic/3DSmoothNet"> <i class="fa fa-github" style="font-size:24px"></i> Code &nbsp&nbsp</a> 
	<a href="javascript:toggleBibtex('gojcic2019perfect')"> BibTeX </a> </p>
	</div>

	<div id="bib_gojcic2019perfect" class="bibtex noshow">
<pre>
@inproceedings{gojcic2019perfect,
  title={The perfect match: 3d point cloud matching with smoothed densities},
  author={Gojcic, Zan and Zhou, Caifa and Wegner, Jan D and Wieser, Andreas},
  booktitle={IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages={5545--5554},
  year={2019}
}
</pre>
</div>
  <br>
  
		<div class="publication_entry">
	<img src="/images/publications/JISDM.jpg" border= "5px solid" width="250px" height="150px" alt="Teaser" align="left" style="margin:5px 8px 8px 5px; border: #aaaaaa 3px solid">
	<p class="title_font"> Robust pointwise correspondences for point cloud based deformation monitoring of natural scenes </p>
	<p class="authors_font"> Zan Gojcic,  <a href="https://gseg.igp.ethz.ch/people/scientific-assistance/caifa-zhou.html" target="_blank"> Caifa Zhou, </a> <a href="https://gseg.igp.ethz.ch/people/group-head/prof-dr--andreas-wieser.html" target="_blank">  Andreas Wieser</a></p>
	<p class="authors_font"> Joint international symposium on deformation monitoring (JISDM), 2019 </p>
	<p class="authors_font"> Best oral presentation award. </p>
	<p class="body_font"> Areal-based deformation monitoring based on point clouds can be a very valuable alternative to the estab-lished point-based monitoring. However, due to naively establishing the pointwise correspondences, established deformation analysis approaches for point clouds do not expose the true 3D changes in parts, which actually did change. Herein we extend  the  recently proposed algorithms  that establish pointwise correspondences in  the feature  space,  with  a  neural  network  based  outlier detection algorithm capable of classifying the putative pointwise correspondences into inliers and outliers based on information only extracted from the point clouds. We demonstrate the proposed approach on two data sets, including a real case data set of a landslide located in the Swiss Alps. We show that while the traditional approaches greatly underestimate the magnitude of the displacements, our approach can correctly estimate the true 3D displacement vectors.</p>
	<p style="font-family: 'Lato'; sans-serif; font-size: 14px; font-weight: bold; line-height: 16px; margin-top: 10px; margin-left: 5px;; margin-bottom: 5px"> <a href="http://www.researchgate.net/profile/Zan_Gojcic/publication/333221444_Robust_pointwise_correspondences_for_point_cloud_based_deformation_monitoring_of_natural_scenes/links/5ce2cfef92851c4eabb0aa39/Robust-pointwise-correspondences-for-point-cloud-based-deformation-monitoring-of-natural-scenes.pdf"> <i class="fa fa-file-pdf-o" style="font-size:24px;color:red"></i> PDF &nbsp&nbsp </a> <a href="javascript:toggleBibtex('gojcic2019robust')"> BibTeX </a> </p>
	</div>
		<div id="bib_gojcic2019robust" class="bibtex noshow">
<pre>
@inproceedings{gojcic2019robust,
  title={Robust pointwise correspondences for point cloud based deformation monitoring of natural scenes},
  author={Gojcic, Zan and Zhou, Caifa and Wieser, Andreas},
  booktitle={4th Joint International Symposium on Deformation Monitoring: JISDM},
  year={2019},
  organization={JISDM}
}
</pre>
</div>
	<br>
			<div class="publication_entry">
	<img src="/images/publications/ISPRS.jpg" border= "5px solid" width="250px" height="150px" alt="Teaser" align="left" style="margin:5px 8px 8px 5px; border: #aaaaaa 3px solid">
	<p class="title_font"> Learned compact local feature descriptor for TLS-based geodetic monitoring of natural outdoor scenes </p>
	<p class="authors_font"> Zan Gojcic,  <a href="https://gseg.igp.ethz.ch/people/scientific-assistance/caifa-zhou.html" target="_blank"> Caifa Zhou, </a> <a href="https://gseg.igp.ethz.ch/people/group-head/prof-dr--andreas-wieser.html" target="_blank">  Andreas Wieser</a></p>
	<p class="authors_font"> ISPRS Annals of the Photogrammetry and Remote Sensing, 2018 </p>
	<p class="body_font"> The advantages of terrestrial laser scanning (TLS) for geodetic monitoring of man-made and natural objects are not yet fully exploited. Herein we address one of the open challenges by proposing feature-based methods for identification of corresponding points in point clouds of two or more epochs. We propose a learned compact feature descriptor tailored for point clouds of natural outdoor scenes obtained using TLS. We evaluate our method both on a benchmark data set and on a specially acquired outdoor dataset resembling a simplified monitoring scenario where we successfully estimate 3D displacement vectors of a rock that has been displaced between the scans. We show that the proposed descriptor has the capacity to generalize to unseen data and achieves state-of-the-art performance while being time efficient at the matching step due the low dimension.</p>
	<p style="font-family: 'Lato'; sans-serif; font-size: 14px; font-weight: bold; line-height: 16px; margin-top: 10px; margin-left: 5px;; margin-bottom: 5px"> <a href="https://www.research-collection.ethz.ch/bitstream/handle/20.500.11850/311866/1/isprs-annals-IV-2-113-2018.pdf"> <i class="fa fa-file-pdf-o" style="font-size:24px;color:red"></i> PDF &nbsp&nbsp </a>
	<a href="javascript:toggleBibtex('gojcic2018annals')"> BibTeX </a> </p>
	</div>
		<div id="bib_gojcic2018annals" class="bibtex noshow">
<pre>
@article{gojcic2018annals,
  title={Learned compact local feature descriptor for TLS-based geodetic monitoring of natural outdoor scenes},
  author={Gojcic, Zan and Zhou, Caifa and Wieser, Andreas},
  journal={International Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
  volume={4},
  pages={113--120},
  year={2018}
}
</pre>
</div>
	<br>
			<div class="publication_entry">
	<img src="/images/publications/INGEO.jpg" border= "5px solid" width="250px" height="150px" alt="Teaser" align="left" style="margin:5px 8px 8px 5px; border: #aaaaaa 3px solid">
	<p class="title_font"> Synchronization routine for real-time synchronization of robotic total stations </p>
	<p class="authors_font"> Zan Gojcic,  <a href="https://www.tugraz.at/institutes/igms/institute/igms-team/" target="_blank"> Slaven Kalenjuk, </a> <a href="https://www.tugraz.at/en/institutes/igms/institute/igms-team/werner-lienhart/" target="_blank">  Werner Lienhart</a></p>
	<p class="authors_font"> ISPRS Annals of the Photogrammetry and Remote Sensing, 2018 </p>
	<p class="body_font"> Large geodetic projects often require measurement configurations with multiple robotic totalstations (RTSs). In order to combine the observables of the spatially separated RTSs, a
	common time frame has to be established. In this paper, we introduce a novel synchronization
	routine for relative, real-time synchronization of multiple RTSs. The proposed routine,
	consisting of two main steps, is independent of ambient conditions and requires no additional
	hardware.In the first part, we analyze the characteristics of the RTS's internal time at stable
	meteorological conditions by comparing it to the reference time established using a dedicated
	GNSS receiver. Referring to the findings, we propose a calibration procedure for the
	temperature calibration of the RTS's internal time. We determine the drift rate at different
	temperatures in dedicated experiments within a climate chamber and derive a calibration
	function from this data.In the second part, we apply the calibration function in practical measurements and show its applicability for selected RTSs at variable temperatures.</p>
	<p style="font-family: 'Lato'; sans-serif; font-size: 14px; font-weight: bold; line-height: 16px; margin-top: 10px; margin-left: 5px;; margin-bottom: 5px"> <a href="http://fig.net/resources/proceedings/2017/2017_10_INGEO/44PR_TS4-4_Gojcic.pdf"> <i class="fa fa-file-pdf-o" style="font-size:24px;color:red"></i> PDF &nbsp&nbsp </a>
	<a href="javascript:toggleBibtex('gojcic2017synchronization')"> BibTeX </a> </p>
	</div>
		<div id="bib_gojcic2017synchronization" class="bibtex noshow">
<pre>
@inproceedings{gojcic2017synchronization,
  title={Synchronization routine for real-time synchronization of robotic total stations},
  author={Gojcic, Zan and Kalenjuk, Slaven and Lienhart, Werner},
  booktitle={INGENEO 2017: Proceedings of the 7th International Conference on Engineering Surveying},
  pages={83--91},
  year={2017}
}
</pre>
</div>
  </article>
</div><!-- /#index -->
<div class="footer-wrap">
  <footer>
    {% include _footer.html %}
  </footer>
</div><!-- /.footer-wrap -->

{% include _scripts.html %}          

</body>
</html>
